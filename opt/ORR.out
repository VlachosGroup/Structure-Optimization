Adding dependency `gcc/4.6.3` to your environment
Adding dependency `openssl/1.0.2-gcc-4.6.3` to your environment
Adding dependency `netlib-lapack/3.7.0-gcc-4.6.3` to your environment
Adding dependency `python/2.7.8-1` to your environment
Adding package `python-networkx/python2.7.8` to your environment
Adding dependency `gcc/4.6.3` to your environment
Adding dependency `openssl/1.0.2-gcc-4.6.3` to your environment
Adding dependency `netlib-lapack/3.7.0-gcc-4.6.3` to your environment
Adding dependency `python/2.7.8-1` to your environment
Adding package `python-networkx/python2.7.8` to your environment
ERROR: unable to add versioned package: python/2.7.8 conflicts with version: python/2.7.8-1
Adding package `openmpi/1.6.3-gcc` to your environment
ERROR: unable to add versioned package: python/2.7.8 conflicts with version: python/2.7.8-1
ERROR: unable to add versioned package: python/2.7.8 conflicts with version: python/2.7.8-1

Running on node01-28 with job id 235758

Using 16 processors.
85.0117866706

real	33m52.187s
user	299m51.394s
sys	8m14.458s
Adding dependency `gcc/4.6.3` to your environment
Adding dependency `openssl/1.0.2-gcc-4.6.3` to your environment
Adding dependency `netlib-lapack/3.7.0-gcc-4.6.3` to your environment
Adding dependency `python/2.7.8-1` to your environment
Adding package `python-networkx/python2.7.8` to your environment
Adding dependency `gcc/4.6.3` to your environment
Adding dependency `openssl/1.0.2-gcc-4.6.3` to your environment
Adding dependency `netlib-lapack/3.7.0-gcc-4.6.3` to your environment
Adding dependency `python/2.7.8-1` to your environment
Adding package `python-networkx/python2.7.8` to your environment
ERROR: unable to add versioned package: python/2.7.8 conflicts with version: python/2.7.8-1
Adding package `openmpi/1.6.3-gcc` to your environment
ERROR: unable to add versioned package: python/2.7.8 conflicts with version: python/2.7.8-1
ERROR: unable to add versioned package: python/2.7.8 conflicts with version: python/2.7.8-1

Running on node01-28 with job id 235958

/home/vlachos/mpnunez/python_packages/sklearn/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:1306: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
/home/vlachos/mpnunez/python_packages/sklearn/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.
  % self.max_iter, ConvergenceWarning)
Using 16 processors.
0 generations have elapsed.
Evaluation time: 0.329507827759 seconds
Iteration 1, loss = 14.40559167
Iteration 2, loss = 13.94068035
Iteration 3, loss = 13.33249392
Iteration 4, loss = 13.10726694
Iteration 5, loss = 12.88943435
Iteration 6, loss = 12.70064666
Iteration 7, loss = 12.47572203
Iteration 8, loss = 12.24853188
Iteration 9, loss = 12.06902753
Iteration 10, loss = 11.85829577
Iteration 11, loss = 11.67256438
Iteration 12, loss = 11.47724794
Iteration 13, loss = 11.26072540
Iteration 14, loss = 11.07039744
Iteration 15, loss = 10.88851670
Iteration 16, loss = 10.71214705
Iteration 17, loss = 10.50677606
Iteration 18, loss = 10.31597968
Iteration 19, loss = 10.10948144
Iteration 20, loss = 9.89438937
Iteration 21, loss = 9.69979940
Iteration 22, loss = 9.51679497
Iteration 23, loss = 9.31025220
Iteration 24, loss = 9.10906340
Iteration 25, loss = 8.89885529
Iteration 26, loss = 8.67584145
Iteration 27, loss = 8.47050790
Iteration 28, loss = 8.25155564
Iteration 29, loss = 8.04092555
Iteration 30, loss = 7.83319384
Iteration 31, loss = 7.61860145
Iteration 32, loss = 7.43248707
Iteration 33, loss = 7.21160160
Iteration 34, loss = 7.03781729
Iteration 35, loss = 6.79085551
Iteration 36, loss = 6.59907166
Iteration 37, loss = 6.39130260
Iteration 38, loss = 6.17103200
Iteration 39, loss = 5.97548843
Iteration 40, loss = 5.79892436
Iteration 41, loss = 5.56442232
Iteration 42, loss = 5.36982525
Iteration 43, loss = 5.21526001
Iteration 44, loss = 5.04637399
Iteration 45, loss = 4.83096875
Iteration 46, loss = 4.65885420
Iteration 47, loss = 4.47799380
Iteration 48, loss = 4.31141374
Iteration 49, loss = 4.13026035
Iteration 50, loss = 3.97770938
Iteration 51, loss = 3.82005437
Iteration 52, loss = 3.67196646
Iteration 53, loss = 3.53378026
Iteration 54, loss = 3.43395740
Iteration 55, loss = 3.29769497
Iteration 56, loss = 3.16954559
Iteration 57, loss = 2.99268071
Iteration 58, loss = 2.89707467
Iteration 59, loss = 2.79341090
Iteration 60, loss = 2.66903490
Iteration 61, loss = 2.56641190
Iteration 62, loss = 2.44908599
Iteration 63, loss = 2.35322058
Iteration 64, loss = 2.27511804
Iteration 65, loss = 2.18094740
Iteration 66, loss = 2.10023563
Iteration 67, loss = 2.01266353
Iteration 68, loss = 1.93889872
Iteration 69, loss = 1.87070760
Iteration 70, loss = 1.81121331
Iteration 71, loss = 1.74518085
Iteration 72, loss = 1.70674550
Iteration 73, loss = 1.62874593
Iteration 74, loss = 1.57549258
Iteration 75, loss = 1.52675603
Iteration 76, loss = 1.47381134
Iteration 77, loss = 1.42799775
Iteration 78, loss = 1.38531332
Iteration 79, loss = 1.35314108
Iteration 80, loss = 1.30702962
Iteration 81, loss = 1.26971127
Iteration 82, loss = 1.23152126
Iteration 83, loss = 1.20051147
Iteration 84, loss = 1.16122362
Iteration 85, loss = 1.12782392
Iteration 86, loss = 1.11027378
Iteration 87, loss = 1.06822302
Iteration 88, loss = 1.04768045
Iteration 89, loss = 1.02823922
Iteration 90, loss = 0.99383550
Iteration 91, loss = 0.97260904
Iteration 92, loss = 0.94363827
Iteration 93, loss = 0.91009785
Iteration 94, loss = 0.89167616
Iteration 95, loss = 0.86521621
Iteration 96, loss = 0.84340227
Iteration 97, loss = 0.82298422
Iteration 98, loss = 0.80277489
Iteration 99, loss = 0.78163371
Iteration 100, loss = 0.76594380
Iteration 101, loss = 0.74420773
Iteration 102, loss = 0.72620328
Iteration 103, loss = 0.71018226
Iteration 104, loss = 0.69997379
Iteration 105, loss = 0.67971088
Iteration 106, loss = 0.66522138
Iteration 107, loss = 0.64552324
Iteration 108, loss = 0.63576046
Iteration 109, loss = 0.61525826
Iteration 110, loss = 0.60256667
Iteration 111, loss = 0.58828785
Iteration 112, loss = 0.57263922
Iteration 113, loss = 0.56070790
Iteration 114, loss = 0.54669120
Iteration 115, loss = 0.53574248
Iteration 116, loss = 0.52367040
Iteration 117, loss = 0.50721300
Iteration 118, loss = 0.49855036
Iteration 119, loss = 0.48471594
Iteration 120, loss = 0.47254774
Iteration 121, loss = 0.46505558
Iteration 122, loss = 0.45282673
Iteration 123, loss = 0.45573951
Iteration 124, loss = 0.43918176
Iteration 125, loss = 0.43226062
Iteration 126, loss = 0.42310406
Iteration 127, loss = 0.40338768
Iteration 128, loss = 0.39620242
Iteration 129, loss = 0.38782637
Iteration 130, loss = 0.37594089
Iteration 131, loss = 0.37765169
Iteration 132, loss = 0.35425709
Iteration 133, loss = 0.34457133
Iteration 134, loss = 0.33232166
Iteration 135, loss = 0.32467347
Iteration 136, loss = 0.31737690
Iteration 137, loss = 0.31374897
Iteration 138, loss = 0.30103423
Iteration 139, loss = 0.29452892
Iteration 140, loss = 0.28603910
Iteration 141, loss = 0.27871580
Iteration 142, loss = 0.27366416
Iteration 143, loss = 0.26964009
Iteration 144, loss = 0.25776138
Iteration 145, loss = 0.25348884
Iteration 146, loss = 0.24990210
Iteration 147, loss = 0.24880556
Iteration 148, loss = 0.23508791
Iteration 149, loss = 0.22805947
Iteration 150, loss = 0.23207182
Iteration 151, loss = 0.21751976
Iteration 152, loss = 0.20946786
Iteration 153, loss = 0.20259267
Iteration 154, loss = 0.19852154
Iteration 155, loss = 0.19288444
Iteration 156, loss = 0.19142107
Iteration 157, loss = 0.18994806
Iteration 158, loss = 0.18811399
Iteration 159, loss = 0.17389161
Iteration 160, loss = 0.16970684
Iteration 161, loss = 0.16372458
Iteration 162, loss = 0.16135585
Iteration 163, loss = 0.15540691
Iteration 164, loss = 0.15160673
Iteration 165, loss = 0.15149984
Iteration 166, loss = 0.14493744
Iteration 167, loss = 0.14034261
Iteration 168, loss = 0.13627976
Iteration 169, loss = 0.13410166
Iteration 170, loss = 0.12942001
Iteration 171, loss = 0.12668721
Iteration 172, loss = 0.12303374
Iteration 173, loss = 0.12000995
Iteration 174, loss = 0.11773060
Iteration 175, loss = 0.11409968
Iteration 176, loss = 0.11334180
Iteration 177, loss = 0.11293173
Iteration 178, loss = 0.10646240
Iteration 179, loss = 0.10322673
Iteration 180, loss = 0.10098762
Iteration 181, loss = 0.09957962
Iteration 182, loss = 0.09974144
Iteration 183, loss = 0.09711145
Iteration 184, loss = 0.09670767
Iteration 185, loss = 0.09054835
Iteration 186, loss = 0.08767046
Iteration 187, loss = 0.08700444
Iteration 188, loss = 0.08343498
Iteration 189, loss = 0.08329496
Iteration 190, loss = 0.08442275
Iteration 191, loss = 0.08072776
Iteration 192, loss = 0.07943136
Iteration 193, loss = 0.07478411
Iteration 194, loss = 0.07209177
Iteration 195, loss = 0.07089159
Iteration 196, loss = 0.06935304
Iteration 197, loss = 0.07049884
Iteration 198, loss = 0.06869896
Iteration 199, loss = 0.06543411
Iteration 200, loss = 0.06372298
Neural network training time: 13.0343871117 seconds
Traceback (most recent call last):
  File "nn_test.py", line 91, in <module>
    plot_pop_SO(np.array(fits), fname = 'Generation_' + str(gen) + '.png', title = 'Generation ' + str(gen))
  File "/mnt/krusty-krab/home/mpnunez/Github/Structure-Optimization/opt/plotpop.py", line 45, in plot_pop_SO
    plt.hist(data)
  File "/opt/shared/python/2.7.8-1/lib/python2.7/site-packages/matplotlib/pyplot.py", line 2332, in hist
    ret = ax.hist(x, bins, range, normed, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, **kwargs)
  File "/opt/shared/python/2.7.8-1/lib/python2.7/site-packages/matplotlib/axes.py", line 7660, in hist
    m, bins = np.histogram(x[i], bins, weights=w[i], **hist_kwargs)
TypeError: histogram() got an unexpected keyword argument 'new'

real	0m17.458s
user	0m18.034s
sys	0m1.535s
